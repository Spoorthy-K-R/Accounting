{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download 10-Ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads 10-K URLs from an a list file and downloads the corresponding filings from the SEC's EDGAR system. Here's a breakdown of how the code works, along with how to modify it for your specific needs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- requests: Used to download the 10-K filings from the SEC website.\n",
    "- os: Used to create directories and manage file paths.\n",
    "- time: Used for rate-limiting to avoid sending too many requests in a short time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function to Extract 10-K URLs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_10k_urls(file_path):\n",
    "    urls = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('10-K'):\n",
    "                parts = line.split(\",\")\n",
    "                # Assuming the URL is the last part of the line\n",
    "                url = parts[-1].strip()\n",
    "                urls.append(f\"https://www.sec.gov/Archives/{url}\")\n",
    "                # if len(urls) == 10:  # Limit to first 10 URLs\n",
    "                #    break\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- extract_10k_urls(file_path): This function opens the list file and looks for lines starting with 10-K (indicating that the filing is a 10-K form). It assumes the URL is the last part of the line and adds it to the list of URLs. You can uncomment the lines to limit the extraction to the first 10 URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function to Download 10-K Filings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_10k_files(urls, folder_name):\n",
    "    os.makedirs(folder_name, exist_ok=True)  # Ensure the folder exists\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'TestBot +spoo@test.ai' # uPDATE WITH YOUR ACTUAL DETAILS\n",
    "    }    \n",
    "\n",
    "    request_interval = 1.0 / 10  # Interval between requests, in seconds (10 requests per second)\n",
    "    \n",
    "    for i, url in enumerate(urls, start=1):\n",
    "        start_time = time.time()  # Start time of the request\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the file name from the URL\n",
    "            url_path = url.split('/')\n",
    "            file_name = url_path[-1]  # The last part of the path is the file name\n",
    "            \n",
    "            # Save the file in the specified folder\n",
    "            file_path = os.path.join(folder_name, file_name)\n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Downloaded {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download file from {url}. Status Code: {response.status_code}\")\n",
    "        \n",
    "        # Rate-limiting to avoid overwhelming the SEC's servers\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time < request_interval:\n",
    "            time.sleep(request_interval - elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- download_10k_files(urls, folder_name): This function downloads each URL in the list of URLs and saves the 10-K filings to the specified folder. It uses rate-limiting to ensure no more than 10 requests per second are sent to the SECâ€™s servers.\n",
    "- If the download is successful (status_code == 200), the file is saved with the name extracted from the URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Set File Path and Download 10-K Filings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded /Users/spoorthy/Projects/Accounting/10K/0001770787-24-000017.txt\n",
      "Downloaded /Users/spoorthy/Projects/Accounting/10K/0001213900-24-025262.txt\n",
      "Downloaded /Users/spoorthy/Projects/Accounting/10K/0000950170-24-038577.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/spoorthy/Projects/Accounting/combined_filtered.csv'  # Update this with the actual path to your forms.idx file\n",
    "urls_10k = extract_10k_urls(file_path)  # Extract URLs for 10-K filings\n",
    "\n",
    "folder_name = \"/Users/spoorthy/Projects/Accounting/10K\"  # Update this with the desired path to store downloaded files\n",
    "download_10k_files(urls_10k, folder_name)  # Download the 10-K filings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set File Path: Replace 'D:/EDGAR/Forms/forms.idx' with the actual file path to your forms.idx file.\n",
    "- Set Folder Path: Replace 'D:/EDGAR/10K' with the desired path where the 10-K filings should be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final Notes:\n",
    "- Make sure you have the correct paths for the forms.idx file and the folder where you want to save the downloaded 10-K filings. The code includes a rate limiter to ensure it follows SEC guidelines for web scraping, which is important to avoid being blocked by the server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
